
- WHAT IS REPLICATION FACTOR IN HADOOP?
	> IT IS BASICALLY THE NUMBER OF TIMES HADOOP FRAMEWORK REPLICATE EACH AND EVERY DATA BLOCK. 	> BLOCK IS REPLICATED TO PROVIDE FAULT TOLERANCE. 
	> THE DEFAULT REPLICATION FACTOR IS 3 WHICH CAN BE CONFIGURED AS PER THE REQUIREMENT

- WHAT IS THE IDEAL REPLICATION FACTOR?
	> REPLICATION FACTOR 3 MEANS YOU CAN AFFORD TO GO ONE NODE GO OUT OF COMMISSION AND STILL HAVE FAULT TOLERANCE WITH 2 NODES LEFT.


- HOW INCREASING/DECREASING THE REPLICATION FACTOR WILL IMPACT HADOOP CLUSTER?
	> INCREASING/DECREASING THE REPLICATION FACTOR IN HDFS HAS A IMPACT ON HADOOP CLUSTER PERFORMANCE.
	> NOW LETS THINK OF INCREASING THE REPLICATION FACTOR FROM DEFAULT 3 TO 5,AS THE REPLICATION FACTOR INCREASES, THE NAMENODE NEEDS TO STORE THE MORE METADATA ABOUT THE REPLICATED COPIES.
SO AT ONE PARTICULAR TIME NAMENODE MIGHT FACE HEAVY LOAD ON IT, AND EVEN IT MIGHT NOT BE ABLE TO PROCESS THE REQUEST QUICKLY
	> ALSO WE NEEDS TO CONSIDER THE NAMENODE’S CAPACITY(LIKE WHETHER ITS A HDD OR SSD, PROCESSOR ETC.,) IN ORDER TO SET THE REPLICATION FACTOR.
	> SO AS THE REPLICATION FACTOR INCREASES, WE WILL GET BETTER DATA RELIABILITY, WHICH IS GOOD BUT AT THE COST OF PERFORMANCE. IF YOU HAVE ENOUGH RESOURCES(STORAGE AND PROCESSING) THEN INCREASING THE REPLICATION FACTOR WILL CERTAINLY BENEFIT.


- WHAT IS A BLOCK AND DATA BLOCK SIZE IN HDFS?
	> HADOOP IS KNOWN FOR ITS RELIABLE STORAGE. HADOOP HDFS CAN STORE DATA OF ANY SIZE AND FORMAT.
	> HDFS IN HADOOP DIVIDES THE FILE INTO SMALL SIZE BLOCKS CALLED DATA BLOCKS. 
	> IN HDFS 3.X, THE SIZE OF THESE HDFS DATA BLOCKS IS 128 MB BY DEFAULT. WE CAN CONFIGURE THE BLOCK SIZE AS PER OUR REQUIREMENT BY CHANGING THE DFS.BLOCK.SIZE PROPERTY IN HDFS-SITE.XML
	> EXAMPLE : SUPPOSE WE HAVE A FILE OF SIZE 612 MB, AND WE ARE USING THE DEFAULT BLOCK CONFIGURATION (128 MB). THEREFORE FIVE BLOCKS ARE CREATED, THE FIRST FOUR BLOCKS ARE 128 MB IN SIZE, AND THE FIFTH BLOCK IS 100 MB IN SIZE (128*4+100=612).

- ADVANTAGES OF HADOOP DATA BLOCKS ?
	> NO LIMITATION ON THE FILE SIZE - A FILE CAN BE LARGER THAN ANY SINGLE DISK IN THE NETWORK.
	> SIMPLICITY OF STORAGE SUBSYSTEM - SINCE BLOCKS ARE OF FIXED SIZE, WE CAN EASILY CALCULATE THE NUMBER OF BLOCKS THAT CAN BE STORED ON A GIVEN DISK. THUS, PROVIDE SIMPLICITY TO THE STORAGE SUBSYSTEM.
	> FAULT TOLERANCE AND HIGH AVAILABILITY - BLOCKS ARE EASY TO REPLICATE BETWEEN DATANODES THUS, PROVIDE FAULT TOLERANCE AND HIGH AVAILABILITY.
	> ELIMINATING METADATA CONCERNS - SINCE BLOCKS ARE JUST CHUNKS OF DATA TO BE STORED, WE DON’T NEED TO STORE FILE METADATA (SUCH AS PERMISSION INFORMATION) WITH THE BLOCKS, ANOTHER SYSTEM CAN HANDLE METADATA SEPARATELY.


MORE INFO : IN HDFS THE BLOCK SIZE CONTROLS THE LEVEL OF REPLICATION DECLUSTERING. THE LOWER THE BLOCK SIZE YOUR BLOCKS ARE MORE EVENLY DISTRIBUTED ACROSS THE DATANODES. THE HIGHER THE BLOCK SIZE YOUR DATA ARE POTENTIALLY LESS EQUALLY DISTRIBUTED IN YOUR CLUSTER.


- DIFFERENCE BETWEEN TRUNCATE AND DROP
	> TRUNCATE : DELETE ONLY THE DATA
	> DROP : DELETE EVERYTHING INCLUDING DATA, SCHEMA, METADATA

- WHY HIVE TABLES DO NOT HAVE PRIMARY KEYS?
	> IT IS A DATAWAREHOUSE
	> BULK DATA LOAD
	> NOT A TRANDACTIONAL DATABASE TO PERFORM CRUD OPERATION

- WHY DERBY DATABASE IS NOT USED WITH HIVE ENGINE?

- WHAT ARE THE ADVANTAGES OF USING EXTERNAL DATABASE FOR HIVE ?

- WHEN TO USE EXTERNAL TABLE?
	> WHEN WE CAN NOT AFFORD LOSS DATA, BUT CAN COMPROMISE ON PERFORMANCE/RUNTIME
	> DATA IS MORE SECURED AS WE ADD A POINTER TO THE ORIGINAL DATA

- WHEN TO USE INTERNAL TABLE?
	> WHEN WE CAN AFFORD DATA LOSS DUE TO DATA MOVEMENT, BUT CAN NOT COMPROMISE ON PERFORMANCE
	> DATA IS LOCATED IN THE HIVE ENVIRONMENT LOCALLY, SO LESS NETWORK LATENCY

- EXPLAIN DAG
	> DIRECTED ACYCLIC GRAPH (DAG) GRAPH
	> BACKBONE OF FAULT TOLERATE EXECUTION ENGINE [EXAMPLE: SPARK]
	> NO CYCLIC OPERATION, RATHER LINEAR IN NATURE
	> EVERY STEP IS AWARE OF IT'S PREVIOUS STEP

- STEPS TO ALLOCATE CONTAINERS
	> SUBMIT THE JOB FOR EXECUTION
	> ANALYZE THE TOTAL SIZE OF DATA WHICH NEEDS TO BE PROCESSED
	> YARN WILL ACQUIRE RESOURCES BASED ON DATA SIZE FROM THE CLUSTER
		> ALLOCATE NO. OF CONTAINERS BASED AS NEEDED
		> 1 CONTAINER CAN RUN MULTIPLE TASKS BASED ON VIRTUAL CORES AVAILABLE FOR THE CONTAINER
	> MAPPER TASKS WILL BE CREATED
		> MAP TASKS -> KEY/VALUE PAIR
	> REDUCER WILL AGGREGATE THE DATA BASED ON KEYS TO GENERATE THE FINAL RESULTS

- EXAMPLAIN HIVE STORAGE (WAREHOUSE STORE)
	> CREATE A WAREHOUSE STORE DIRECTORY IN HDFS "/USER/HIVE/<FOLDER NAME>"
	> DATA AS A MULTI-PART WILL BE STORED IN THE DIRECTORY AS CREATED
	> THIS DATA STORE IS NOT FOR META DATA, RATHER FOR ACTUAL DATA

- WHAT IS HIVE?
	> A DATA WAREHOUSING SERVICE
	> CAN BOTH CONPUTE & STORE

- WHAT IS SPARK?
	> A COMPUTATIONAL ENGINE
	> APACHE SPARK IS AN ANALYTICS FRAMEWORK FOR LARGE SCALE DATA PROCESSING. 
	> IT COMPUTE DATA, BUT DO NOT STORE PERMANENTLY

- WHAT IS RDD? 
	> https://intellipaat.com/blog/tutorial/spark-tutorial/programming-with-rdds/
	> RDDS ARE THE MAIN LOGICAL DATA UNITS IN SPARK. THEY ARE A DISTRIBUTED COLLECTION OF OBJECTS, WHICH ARE STORED IN MEMORY OR ON DISKS OF DIFFERENT MACHINES OF A CLUSTER. 

- FEATURES OF AN RDD IN SPARK?
	> RESILIENCE: RDDS TRACK DATA LINEAGE INFORMATION TO RECOVER LOST DATA, AUTOMATICALLY ON FAILURE. IT IS ALSO CALLED FAULT TOLERANCE.
DISTRIBUTED: DATA PRESENT IN AN RDD RESIDES ON MULTIPLE NODES. IT IS DISTRIBUTED ACROSS DIFFERENT NODES OF A CLUSTER.
	> LAZY EVALUATION: DATA DOES NOT GET LOADED IN AN RDD EVEN IF YOU DEFINE IT. TRANSFORMATIONS ARE ACTUALLY COMPUTED WHEN YOU CALL ACTION, SUCH AS COUNT OR COLLECT, OR SAVE THE OUTPUT TO A FILE SYSTEM.
	> IMMUTABILITY: DATA STORED IN AN RDD IS IN THE READ-ONLY MODE━ YOU CANNOT EDIT THE DATA WHICH IS PRESENT IN THE RDD. BUT YOU CAN CREATE NEW RDDS BY PERFORMING TRANSFORMATIONS ON THE EXISTING RDDS.
	> IN-MEMORY COMPUTATION: AN RDD STORES ANY IMMEDIATE DATA THAT IS GENERATED IN THE MEMORY (RAM) THAN ON THE DISK SO THAT IT PROVIDES FASTER ACCESS.
	> PARTITIONING: PARTITIONS CAN BE DONE ON ANY EXISTING RDD TO CREATE LOGICAL PARTS THAT ARE MUTABLE. YOU CAN ACHIEVE THIS BY APPLYING TRANSFORMATIONS TO THE EXISTING PARTITIONS.

- WHAT IS CHECKPOINTING?
	> IT MEANS HAVING AN UPDATED FS IMAGE IN THE SECONDARY NODE AT REGULAR INTERVAL

- WHY FS IMAGE AND EDIT LOG ARE MAINTAINED IN THE SECONDARY NODE?
	> EDIT LOGS ARE STORED IN RAM, FS IMAGES ARE STORED ON DISK (EDIT LOG -> FS IMAGE)
	> FS IMAGE AND EDIT LOG ARE MAINTAINED IN THE SECONDARY NODE FOR BACKUP PURPOSES
	> IF NAME NODE GOES DOWN FOR SOME REASON LOOSES THE TRACK OF DATA, IT GET THE LATEST FS IMAGE FROM THE SECONDARY NODE AND CONTINUE FROM THERE

- HOW TO LIMIT THE SIZE OF AN EDIT LOG?
	> WE MUST CONTROL THE SIZE OF EDIT LOG AS IT'S DATA STORES IN RAM
	> REDUCING THE TIME FOR CHECKPOINTING WILL REDUCE THE SIZE OF IN-MEMORY DATA THROUGH CONFIGURATION
	> HENCE, MINIMILISTIC EDIT LOGS CAN BE RETAINED IN THE RAM 
	> CONTINUOUSLY GETS MERGE INTO FS IMAGE

	[NOTE: INCREASING RAM MAY NOT BE AN OPTION]

- WHAT IS SCHEMA-ON-READ (HADOOP)?
	> IN HIVE, THE DATA SCHEMA IS NOT VERIFIED DURING THE LOAD TIME, RATHER IT IS VERIFIED WHILE PROCESSING THE QUERY. HENCE THIS PROCESS IN HIVE CALLED SCHEMA-ON-READ.
	> SCHEMA-ON-READ HELPS IN VERY FAST INITIAL DATA LOAD, SINCE THE DATA DOES NOT HAVE TO FOLLOW ANY INTERNAL SCHEMA(INTERNAL DATABASE FORMAT) TO READ OR PARSE OR SERIALIZE, AS IT IS JUST A COPY/MOVE OF A FILE.
	> THIS TYPE OF MOVEMENT OF DATA IS MORE FLEXIBLE INCASE OF HUGE DATA OR HAVING TWO SCHEMAS FOR SAME UNDERLYING DATA.
	> SO, IN SCENARIOS OF LARGE DATA LOAD OR WHERE THE SCHEMA IS NOT KNOWN AT LOAD TIME AND THERE ARE NO INDEXES TO APPLY, AS THE QUERY IS NOT FORMULATED, SCHEMA-ON-READ(PROPERTY OF HIVE) IS MORE EFFICIENT THAN SCHEMA-ON-WRITE.


- WHAT IS SCHEMA-ON-WRITE (RDBMS)?
	> IN TRADITIONAL DATABASES, THE TABLE’S SCHEMA IS IMPOSED DURING THE DATA LOAD TIME, IF THE DATA BEING LOADED DOES NOT CONFORM TO THE SCHEMA THEN THE DATA LOAD IS REJECTED, THIS PROCESS IS KNOW AS SCHEMA-ON-WRITE. HERE THE DATA IS BEING CHECKED AGAINST THE SCHEMA WHEN WRITTEN INTO THE DATABASE (DURING DATA LOAD)
	> SCHEMA-ON-WRITE HELPS IN FASTER PERFORMANCE OF THE QUERY, AS THE DATA IS ALREADY LOADED IN A PARTICULAR FORMAT AND IT IS EASY TO LOCATE THE COLUMN INDEX OR COMPRESS THE DATA. HOWEVER, IT TAKES LONGER TIME TO LOAD DATA INTO THE DATABASE

- HOW IT IS DECIDED THAT WHICH RECORD WILL GO TO WHICH PARTICULAR REDUCER OF MAPREDUCE IN HADOOP?
	> BASED ON HASH VALUE

